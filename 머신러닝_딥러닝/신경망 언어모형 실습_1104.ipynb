{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6571839836309871380\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2254123828\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6395822226982723625\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 텐서 GPU 설정\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://github.com/euphoris/datasets/raw/master/imdb.zip')\n",
    "\n",
    "with open('imdb.zip', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3         Very little music or anything to speak of.          0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('imdb.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = tf.keras.preprocessing.text.Tokenizer(num_words=2000, oov_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.fit_on_texts(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 1,\n",
       " 'the': 2,\n",
       " 'and': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'is': 6,\n",
       " 'this': 7,\n",
       " 'i': 8,\n",
       " 'it': 9,\n",
       " 'to': 10,\n",
       " 'in': 11,\n",
       " 'was': 12,\n",
       " 'movie': 13,\n",
       " 'film': 14,\n",
       " 'that': 15,\n",
       " 'for': 16,\n",
       " 'as': 17,\n",
       " 'but': 18,\n",
       " 'with': 19,\n",
       " 'one': 20,\n",
       " 'on': 21,\n",
       " 'you': 22,\n",
       " 'are': 23,\n",
       " 'not': 24,\n",
       " 'bad': 25,\n",
       " \"it's\": 26,\n",
       " 'very': 27,\n",
       " 'all': 28,\n",
       " 'just': 29,\n",
       " 'so': 30,\n",
       " 'good': 31,\n",
       " 'at': 32,\n",
       " 'an': 33,\n",
       " 'be': 34,\n",
       " 'there': 35,\n",
       " 'about': 36,\n",
       " 'have': 37,\n",
       " 'by': 38,\n",
       " 'like': 39,\n",
       " 'from': 40,\n",
       " 'if': 41,\n",
       " 'acting': 42,\n",
       " 'time': 43,\n",
       " 'out': 44,\n",
       " 'his': 45,\n",
       " 'or': 46,\n",
       " 'really': 47,\n",
       " 'great': 48,\n",
       " 'even': 49,\n",
       " 'he': 50,\n",
       " 'who': 51,\n",
       " 'were': 52,\n",
       " 'has': 53,\n",
       " 'see': 54,\n",
       " 'my': 55,\n",
       " 'characters': 56,\n",
       " 'well': 57,\n",
       " 'most': 58,\n",
       " 'how': 59,\n",
       " 'more': 60,\n",
       " 'no': 61,\n",
       " 'only': 62,\n",
       " 'when': 63,\n",
       " 'ever': 64,\n",
       " '10': 65,\n",
       " 'movies': 66,\n",
       " 'plot': 67,\n",
       " 'story': 68,\n",
       " 'made': 69,\n",
       " 'some': 70,\n",
       " 'they': 71,\n",
       " 'best': 72,\n",
       " 'because': 73,\n",
       " 'your': 74,\n",
       " 'can': 75,\n",
       " 'also': 76,\n",
       " \"don't\": 77,\n",
       " 'films': 78,\n",
       " 'than': 79,\n",
       " 'its': 80,\n",
       " 'script': 81,\n",
       " 'other': 82,\n",
       " 'character': 83,\n",
       " 'would': 84,\n",
       " 'seen': 85,\n",
       " 'way': 86,\n",
       " 'love': 87,\n",
       " 'make': 88,\n",
       " \"didn't\": 89,\n",
       " 'do': 90,\n",
       " 'me': 91,\n",
       " 'watching': 92,\n",
       " 'her': 93,\n",
       " 'which': 94,\n",
       " 'what': 95,\n",
       " 'up': 96,\n",
       " 'any': 97,\n",
       " 'think': 98,\n",
       " 'real': 99,\n",
       " 'could': 100,\n",
       " 'will': 101,\n",
       " 'had': 102,\n",
       " 'every': 103,\n",
       " 'much': 104,\n",
       " 'work': 105,\n",
       " 'too': 106,\n",
       " 'look': 107,\n",
       " 'funny': 108,\n",
       " 'scenes': 109,\n",
       " 'actors': 110,\n",
       " 'better': 111,\n",
       " 'over': 112,\n",
       " 'cast': 113,\n",
       " 'never': 114,\n",
       " 'wonderful': 115,\n",
       " 'little': 116,\n",
       " 'them': 117,\n",
       " 'into': 118,\n",
       " 'watch': 119,\n",
       " 'show': 120,\n",
       " 'everything': 121,\n",
       " 'excellent': 122,\n",
       " 'anyone': 123,\n",
       " 'their': 124,\n",
       " 'totally': 125,\n",
       " 'both': 126,\n",
       " 'here': 127,\n",
       " 'music': 128,\n",
       " 'scene': 129,\n",
       " 'waste': 130,\n",
       " 'people': 131,\n",
       " 'screen': 132,\n",
       " 'go': 133,\n",
       " 'years': 134,\n",
       " 'nothing': 135,\n",
       " 'stupid': 136,\n",
       " 'awful': 137,\n",
       " 'get': 138,\n",
       " 'know': 139,\n",
       " 'still': 140,\n",
       " 'many': 141,\n",
       " 'man': 142,\n",
       " 'art': 143,\n",
       " 'two': 144,\n",
       " 'right': 145,\n",
       " 'say': 146,\n",
       " 'recommend': 147,\n",
       " 'dialogue': 148,\n",
       " 'worth': 149,\n",
       " 'writing': 150,\n",
       " 'pretty': 151,\n",
       " 'after': 152,\n",
       " 'thing': 153,\n",
       " 'again': 154,\n",
       " 'saw': 155,\n",
       " 'thought': 156,\n",
       " 'those': 157,\n",
       " 'life': 158,\n",
       " 'line': 159,\n",
       " \"doesn't\": 160,\n",
       " 'things': 161,\n",
       " 'interesting': 162,\n",
       " 'been': 163,\n",
       " 'such': 164,\n",
       " 'terrible': 165,\n",
       " 'performance': 166,\n",
       " 'being': 167,\n",
       " 'enough': 168,\n",
       " 'beautiful': 169,\n",
       " 'short': 170,\n",
       " 'part': 171,\n",
       " 'did': 172,\n",
       " 'give': 173,\n",
       " \"can't\": 174,\n",
       " 'worst': 175,\n",
       " \"i've\": 176,\n",
       " 'though': 177,\n",
       " 'first': 178,\n",
       " 'ending': 179,\n",
       " 'should': 180,\n",
       " 'end': 181,\n",
       " 'worse': 182,\n",
       " 'she': 183,\n",
       " 'camera': 184,\n",
       " 'find': 185,\n",
       " 'through': 186,\n",
       " 'predictable': 187,\n",
       " 'loved': 188,\n",
       " 'cinematography': 189,\n",
       " 'quite': 190,\n",
       " 'simply': 191,\n",
       " 'actually': 192,\n",
       " \"i'm\": 193,\n",
       " 'actor': 194,\n",
       " 'feeling': 195,\n",
       " 'now': 196,\n",
       " 'liked': 197,\n",
       " 'piece': 198,\n",
       " 'big': 199,\n",
       " 'going': 200,\n",
       " 'played': 201,\n",
       " 'boring': 202,\n",
       " 'however': 203,\n",
       " 'director': 204,\n",
       " 'between': 205,\n",
       " 'highly': 206,\n",
       " 'off': 207,\n",
       " 'drama': 208,\n",
       " 'black': 209,\n",
       " 'almost': 210,\n",
       " 'lot': 211,\n",
       " 'directing': 212,\n",
       " 'does': 213,\n",
       " 'game': 214,\n",
       " 'play': 215,\n",
       " 'least': 216,\n",
       " 'effects': 217,\n",
       " \"that's\": 218,\n",
       " 'enjoyed': 219,\n",
       " 'absolutely': 220,\n",
       " 'these': 221,\n",
       " 'amazing': 222,\n",
       " 'horror': 223,\n",
       " 'definitely': 224,\n",
       " 'understand': 225,\n",
       " 'watched': 226,\n",
       " 'truly': 227,\n",
       " 'job': 228,\n",
       " 'whole': 229,\n",
       " 'we': 230,\n",
       " 'am': 231,\n",
       " 'special': 232,\n",
       " 'white': 233,\n",
       " 'anything': 234,\n",
       " 'kids': 235,\n",
       " 'long': 236,\n",
       " 'cinema': 237,\n",
       " 'where': 238,\n",
       " 'certainly': 239,\n",
       " 'series': 240,\n",
       " 'kind': 241,\n",
       " 'tv': 242,\n",
       " 'found': 243,\n",
       " 'suspense': 244,\n",
       " 'especially': 245,\n",
       " 'myself': 246,\n",
       " 'then': 247,\n",
       " 'fact': 248,\n",
       " 'believe': 249,\n",
       " 'minutes': 250,\n",
       " \"there's\": 251,\n",
       " 'probably': 252,\n",
       " 'got': 253,\n",
       " 'used': 254,\n",
       " 'done': 255,\n",
       " 'old': 256,\n",
       " 'makes': 257,\n",
       " 'mess': 258,\n",
       " 'avoid': 259,\n",
       " 'sucks': 260,\n",
       " 'lines': 261,\n",
       " 'wasted': 262,\n",
       " 'cool': 263,\n",
       " 'budget': 264,\n",
       " 'together': 265,\n",
       " 'far': 266,\n",
       " \"wasn't\": 267,\n",
       " 'written': 268,\n",
       " 'another': 269,\n",
       " 'playing': 270,\n",
       " 'must': 271,\n",
       " '1': 272,\n",
       " 'experience': 273,\n",
       " 'sucked': 274,\n",
       " 'self': 275,\n",
       " 'john': 276,\n",
       " 'action': 277,\n",
       " 'cheap': 278,\n",
       " 'subtle': 279,\n",
       " 'everyone': 280,\n",
       " 'our': 281,\n",
       " 'around': 282,\n",
       " 'each': 283,\n",
       " 'top': 284,\n",
       " 'garbage': 285,\n",
       " 'comedy': 286,\n",
       " 'slow': 287,\n",
       " 'half': 288,\n",
       " 'disappointed': 289,\n",
       " 'poor': 290,\n",
       " 'lacks': 291,\n",
       " 'casting': 292,\n",
       " 'hilarious': 293,\n",
       " 'editing': 294,\n",
       " 'making': 295,\n",
       " 'perfect': 296,\n",
       " 'history': 297,\n",
       " 'rather': 298,\n",
       " 'money': 299,\n",
       " 'crap': 300,\n",
       " 'flick': 301,\n",
       " 'take': 302,\n",
       " 'family': 303,\n",
       " 'entire': 304,\n",
       " 'whatever': 305,\n",
       " 'having': 306,\n",
       " 'performances': 307,\n",
       " 'believable': 308,\n",
       " 'portrayal': 309,\n",
       " 'annoying': 310,\n",
       " 'gives': 311,\n",
       " 'want': 312,\n",
       " 'classic': 313,\n",
       " 'nice': 314,\n",
       " \"i'd\": 315,\n",
       " 'brilliant': 316,\n",
       " 'rent': 317,\n",
       " 'world': 318,\n",
       " 'why': 319,\n",
       " 'horrible': 320,\n",
       " 'few': 321,\n",
       " 'sound': 322,\n",
       " 'incredible': 323,\n",
       " 'him': 324,\n",
       " 'own': 325,\n",
       " 'mostly': 326,\n",
       " 'holes': 327,\n",
       " 'use': 328,\n",
       " 'human': 329,\n",
       " 'memorable': 330,\n",
       " 'seeing': 331,\n",
       " 'gets': 332,\n",
       " 'throughout': 333,\n",
       " 'second': 334,\n",
       " 'style': 335,\n",
       " 'recommended': 336,\n",
       " 'audience': 337,\n",
       " 'clever': 338,\n",
       " 'ridiculous': 339,\n",
       " 'non': 340,\n",
       " 'works': 341,\n",
       " 'guess': 342,\n",
       " 'bit': 343,\n",
       " 'face': 344,\n",
       " 'low': 345,\n",
       " 'since': 346,\n",
       " 'single': 347,\n",
       " 'put': 348,\n",
       " 'production': 349,\n",
       " '\\x96': 350,\n",
       " 'mind': 351,\n",
       " 'amount': 352,\n",
       " 'strong': 353,\n",
       " 'fun': 354,\n",
       " 'enjoy': 355,\n",
       " 'lame': 356,\n",
       " 'away': 357,\n",
       " 'girl': 358,\n",
       " 'tom': 359,\n",
       " 'often': 360,\n",
       " 'word': 361,\n",
       " 'overall': 362,\n",
       " 'gave': 363,\n",
       " 'terrific': 364,\n",
       " 'same': 365,\n",
       " 'hour': 366,\n",
       " 'joy': 367,\n",
       " 'before': 368,\n",
       " 'oh': 369,\n",
       " 'night': 370,\n",
       " \"couldn't\": 371,\n",
       " 'direction': 372,\n",
       " 'cult': 373,\n",
       " 'times': 374,\n",
       " 'beyond': 375,\n",
       " 'lead': 376,\n",
       " 'silent': 377,\n",
       " 'hitchcock': 378,\n",
       " 'thriller': 379,\n",
       " 'new': 380,\n",
       " 'full': 381,\n",
       " 'completely': 382,\n",
       " 'pathetic': 383,\n",
       " 'talk': 384,\n",
       " 'care': 385,\n",
       " 'fails': 386,\n",
       " 'yet': 387,\n",
       " 'said': 388,\n",
       " 'fans': 389,\n",
       " 'solid': 390,\n",
       " 'felt': 391,\n",
       " 'child': 392,\n",
       " 'day': 393,\n",
       " 'started': 394,\n",
       " 'shot': 395,\n",
       " 'mention': 396,\n",
       " 'year': 397,\n",
       " 'different': 398,\n",
       " 'involved': 399,\n",
       " 'period': 400,\n",
       " \"won't\": 401,\n",
       " 'particularly': 402,\n",
       " 'come': 403,\n",
       " 'superb': 404,\n",
       " 'fine': 405,\n",
       " 'fast': 406,\n",
       " 'moving': 407,\n",
       " 'young': 408,\n",
       " 'lost': 409,\n",
       " 'trying': 410,\n",
       " 'song': 411,\n",
       " 'rest': 412,\n",
       " 'hours': 413,\n",
       " 'adorable': 414,\n",
       " 'songs': 415,\n",
       " 'consider': 416,\n",
       " 'tale': 417,\n",
       " \"i'll\": 418,\n",
       " 'true': 419,\n",
       " 'easily': 420,\n",
       " 'something': 421,\n",
       " 'minute': 422,\n",
       " 'level': 423,\n",
       " 'idea': 424,\n",
       " 'mediocre': 425,\n",
       " 'pg': 426,\n",
       " 'point': 427,\n",
       " 'itself': 428,\n",
       " 'perhaps': 429,\n",
       " 'glad': 430,\n",
       " 'lovely': 431,\n",
       " 'hard': 432,\n",
       " '2': 433,\n",
       " 'let': 434,\n",
       " 'utterly': 435,\n",
       " 'convincing': 436,\n",
       " 'book': 437,\n",
       " 'whether': 438,\n",
       " \"they're\": 439,\n",
       " 'follow': 440,\n",
       " 'energy': 441,\n",
       " 'generally': 442,\n",
       " 'pretentious': 443,\n",
       " 'occasionally': 444,\n",
       " 'try': 445,\n",
       " 'extremely': 446,\n",
       " 'back': 447,\n",
       " 'maybe': 448,\n",
       " 'sometimes': 449,\n",
       " 'during': 450,\n",
       " 'chemistry': 451,\n",
       " 'last': 452,\n",
       " 'unfortunately': 453,\n",
       " 'depth': 454,\n",
       " 'imagination': 455,\n",
       " 'barely': 456,\n",
       " 'storyline': 457,\n",
       " 'keep': 458,\n",
       " 'already': 459,\n",
       " 'attempt': 460,\n",
       " 'down': 461,\n",
       " 'mean': 462,\n",
       " 'us': 463,\n",
       " 'without': 464,\n",
       " 'torture': 465,\n",
       " 'premise': 466,\n",
       " 'seem': 467,\n",
       " 'ups': 468,\n",
       " 'age': 469,\n",
       " 'ray': 470,\n",
       " 'usual': 471,\n",
       " 'living': 472,\n",
       " 'themselves': 473,\n",
       " 'else': 474,\n",
       " 'tell': 475,\n",
       " 'visual': 476,\n",
       " 'plain': 477,\n",
       " 'soundtrack': 478,\n",
       " 'trash': 479,\n",
       " 'came': 480,\n",
       " 'either': 481,\n",
       " 'less': 482,\n",
       " 'hope': 483,\n",
       " 'eyes': 484,\n",
       " 'indeed': 485,\n",
       " 'theater': 486,\n",
       " 'rating': 487,\n",
       " 'three': 488,\n",
       " 'huge': 489,\n",
       " 'intelligence': 490,\n",
       " 'intelligent': 491,\n",
       " 'entertaining': 492,\n",
       " 'bored': 493,\n",
       " 'become': 494,\n",
       " 'roles': 495,\n",
       " 'drago': 496,\n",
       " 'looked': 497,\n",
       " 'parts': 498,\n",
       " 'sets': 499,\n",
       " 'stories': 500,\n",
       " 'created': 501,\n",
       " 'role': 502,\n",
       " 'scamp': 503,\n",
       " 'comes': 504,\n",
       " 'place': 505,\n",
       " 'star': 506,\n",
       " 'always': 507,\n",
       " 'insult': 508,\n",
       " 'death': 509,\n",
       " 'may': 510,\n",
       " 'expect': 511,\n",
       " 'serious': 512,\n",
       " 'original': 513,\n",
       " 'james': 514,\n",
       " 'unbelievable': 515,\n",
       " 'costs': 516,\n",
       " 'small': 517,\n",
       " 'dance': 518,\n",
       " 'beginning': 519,\n",
       " 'appreciate': 520,\n",
       " 'easy': 521,\n",
       " 'sure': 522,\n",
       " 'speak': 523,\n",
       " 'head': 524,\n",
       " 'meaning': 525,\n",
       " 'today': 526,\n",
       " 'showed': 527,\n",
       " 'delivers': 528,\n",
       " 'average': 529,\n",
       " 'main': 530,\n",
       " 'greatest': 531,\n",
       " 'gem': 532,\n",
       " 'sea': 533,\n",
       " 'faux': 534,\n",
       " 'important': 535,\n",
       " 'words': 536,\n",
       " 'yes': 537,\n",
       " 'significant': 538,\n",
       " 'picture': 539,\n",
       " 'graphics': 540,\n",
       " 'massive': 541,\n",
       " 'pure': 542,\n",
       " 'brilliance': 543,\n",
       " 'complete': 544,\n",
       " 'while': 545,\n",
       " 'moment': 546,\n",
       " 'told': 547,\n",
       " 'talented': 548,\n",
       " 'stars': 549,\n",
       " 'hill': 550,\n",
       " 'ed': 551,\n",
       " 'quality': 552,\n",
       " 'obviously': 553,\n",
       " 'addition': 554,\n",
       " 'grace': 555,\n",
       " 'negative': 556,\n",
       " 'pointless': 557,\n",
       " 'children': 558,\n",
       " \"wouldn't\": 559,\n",
       " 'dialog': 560,\n",
       " 'shots': 561,\n",
       " \"aren't\": 562,\n",
       " 'given': 563,\n",
       " 'provoking': 564,\n",
       " 'plus': 565,\n",
       " 'paced': 566,\n",
       " 'wind': 567,\n",
       " 'lion': 568,\n",
       " 'acted': 569,\n",
       " 'decent': 570,\n",
       " 'checking': 571,\n",
       " 'touching': 572,\n",
       " 'looking': 573,\n",
       " 'including': 574,\n",
       " 'embarrassing': 575,\n",
       " 'scenery': 576,\n",
       " 'house': 577,\n",
       " 'wish': 578,\n",
       " 'along': 579,\n",
       " 'happened': 580,\n",
       " 'seems': 581,\n",
       " 'mature': 582,\n",
       " 'episode': 583,\n",
       " 'remake': 584,\n",
       " 'fear': 585,\n",
       " 'nobody': 586,\n",
       " 'conflict': 587,\n",
       " 'incredibly': 588,\n",
       " 'possible': 589,\n",
       " 'whatsoever': 590,\n",
       " 'stereotypes': 591,\n",
       " 'cartoon': 592,\n",
       " 'paul': 593,\n",
       " 'women': 594,\n",
       " 'brain': 595,\n",
       " 'left': 596,\n",
       " 'features': 597,\n",
       " 'presents': 598,\n",
       " 'free': 599,\n",
       " 'screenwriter': 600,\n",
       " 'close': 601,\n",
       " 'seemed': 602,\n",
       " \"you'll\": 603,\n",
       " 'indulgent': 604,\n",
       " 'spent': 605,\n",
       " \"isn't\": 606,\n",
       " 'charles': 607,\n",
       " 'remember': 608,\n",
       " 'working': 609,\n",
       " 'attention': 610,\n",
       " 'singing': 611,\n",
       " 'etc': 612,\n",
       " 'bore': 613,\n",
       " 'dancing': 614,\n",
       " 'dvd': 615,\n",
       " 'might': 616,\n",
       " 'theme': 617,\n",
       " 'aerial': 618,\n",
       " 'interest': 619,\n",
       " 'narrative': 620,\n",
       " 'actress': 621,\n",
       " 'called': 622,\n",
       " 'spoilers': 623,\n",
       " 'stunning': 624,\n",
       " 'fx': 625,\n",
       " 'note': 626,\n",
       " 'surprisingly': 627,\n",
       " 'released': 628,\n",
       " 'ranks': 629,\n",
       " 'journey': 630,\n",
       " 'location': 631,\n",
       " 'thoroughly': 632,\n",
       " 'turn': 633,\n",
       " 'memories': 634,\n",
       " 'places': 635,\n",
       " 'billy': 636,\n",
       " 'possibly': 637,\n",
       " 'trilogy': 638,\n",
       " 'favourite': 639,\n",
       " 'awesome': 640,\n",
       " 'earlier': 641,\n",
       " 'directed': 642,\n",
       " 'someone': 643,\n",
       " 'sense': 644,\n",
       " 'games': 645,\n",
       " 'genuine': 646,\n",
       " 'smart': 647,\n",
       " \"haven't\": 648,\n",
       " 'particular': 649,\n",
       " 'stage': 650,\n",
       " 'next': 651,\n",
       " '8': 652,\n",
       " 'super': 653,\n",
       " 'wonderfully': 654,\n",
       " 'actresses': 655,\n",
       " 'exactly': 656,\n",
       " 'shows': 657,\n",
       " 'drive': 658,\n",
       " 'scared': 659,\n",
       " 'enjoyable': 660,\n",
       " 'bought': 661,\n",
       " '90': 662,\n",
       " 'god': 663,\n",
       " 'effective': 664,\n",
       " 'set': 665,\n",
       " 'learn': 666,\n",
       " 'values': 667,\n",
       " 'photography': 668,\n",
       " 'ruthless': 669,\n",
       " 'although': 670,\n",
       " 'war': 671,\n",
       " 'type': 672,\n",
       " 'example': 673,\n",
       " 'feel': 674,\n",
       " 'seriously': 675,\n",
       " 'ready': 676,\n",
       " 'fan': 677,\n",
       " 'angel': 678,\n",
       " 'under': 679,\n",
       " 'coming': 680,\n",
       " '20': 681,\n",
       " 'charming': 682,\n",
       " 'clichés': 683,\n",
       " 'thrown': 684,\n",
       " 'reason': 685,\n",
       " 'scale': 686,\n",
       " 'problems': 687,\n",
       " 'score': 688,\n",
       " 'frightening': 689,\n",
       " 'oscar': 690,\n",
       " 'knew': 691,\n",
       " 'lots': 692,\n",
       " 'space': 693,\n",
       " 'footage': 694,\n",
       " 'course': 695,\n",
       " 'perfectly': 696,\n",
       " 'finally': 697,\n",
       " 'share': 698,\n",
       " 'rate': 699,\n",
       " 'sort': 700,\n",
       " 'conclusion': 701,\n",
       " 'heart': 702,\n",
       " 'race': 703,\n",
       " 'appearance': 704,\n",
       " 'looks': 705,\n",
       " 'happen': 706,\n",
       " 'final': 707,\n",
       " 'unconvincing': 708,\n",
       " 'produced': 709,\n",
       " 'early': 710,\n",
       " 'documentary': 711,\n",
       " 'martin': 712,\n",
       " 'racial': 713,\n",
       " 'mickey': 714,\n",
       " 'watchable': 715,\n",
       " 'weak': 716,\n",
       " 'due': 717,\n",
       " \"film's\": 718,\n",
       " 'recent': 719,\n",
       " \"90's\": 720,\n",
       " 'nonsense': 721,\n",
       " 'fantastic': 722,\n",
       " 'south': 723,\n",
       " \"you're\": 724,\n",
       " 'plays': 725,\n",
       " 'thinking': 726,\n",
       " 'wrong': 727,\n",
       " 'giallo': 728,\n",
       " 'sub': 729,\n",
       " 'emotions': 730,\n",
       " 'ability': 731,\n",
       " 'write': 732,\n",
       " 'writer': 733,\n",
       " 'puppets': 734,\n",
       " 'animation': 735,\n",
       " 'flat': 736,\n",
       " 'whom': 737,\n",
       " 'walked': 738,\n",
       " 'angles': 739,\n",
       " 'became': 740,\n",
       " 'keeps': 741,\n",
       " 'running': 742,\n",
       " 'charm': 743,\n",
       " 'empty': 744,\n",
       " 'jimmy': 745,\n",
       " 'appealing': 746,\n",
       " 'case': 747,\n",
       " 'clearly': 748,\n",
       " 'review': 749,\n",
       " 'sisters': 750,\n",
       " 'terms': 751,\n",
       " 'aspect': 752,\n",
       " 'masterpiece': 753,\n",
       " 'masterpieces': 754,\n",
       " 'ask': 755,\n",
       " 'form': 756,\n",
       " 'imaginable': 757,\n",
       " 'pieces': 758,\n",
       " 'fit': 759,\n",
       " 'create': 760,\n",
       " 'deserves': 761,\n",
       " 'levels': 762,\n",
       " 'canada': 763,\n",
       " 'buy': 764,\n",
       " '13': 765,\n",
       " 'sequel': 766,\n",
       " 'rated': 767,\n",
       " 'interested': 768,\n",
       " 'unfunny': 769,\n",
       " 'joke': 770,\n",
       " 'morgan': 771,\n",
       " 'jonah': 772,\n",
       " 'lazy': 773,\n",
       " 'presence': 774,\n",
       " 'obvious': 775,\n",
       " 'cost': 776,\n",
       " 'despite': 777,\n",
       " 'choice': 778,\n",
       " 'lesser': 779,\n",
       " 'french': 780,\n",
       " 'fall': 781,\n",
       " 'cause': 782,\n",
       " 'regret': 783,\n",
       " 'front': 784,\n",
       " 'whiny': 785,\n",
       " 'future': 786,\n",
       " 'anne': 787,\n",
       " '9': 788,\n",
       " 'reading': 789,\n",
       " 'voice': 790,\n",
       " 'warmth': 791,\n",
       " 'twice': 792,\n",
       " 'delivering': 793,\n",
       " 'honestly': 794,\n",
       " 'unpredictable': 795,\n",
       " 'badly': 796,\n",
       " 'parents': 797,\n",
       " 'alexander': 798,\n",
       " 'artist': 799,\n",
       " 's': 800,\n",
       " 'gonna': 801,\n",
       " 'business': 802,\n",
       " 'needed': 803,\n",
       " 'mouth': 804,\n",
       " 'run': 805,\n",
       " 'superbly': 806,\n",
       " 'turned': 807,\n",
       " 'b': 808,\n",
       " 'list': 809,\n",
       " 'problem': 810,\n",
       " 'heaven': 811,\n",
       " 'lives': 812,\n",
       " 'church': 813,\n",
       " 'uplifting': 814,\n",
       " 'frankly': 815,\n",
       " 'lane': 816,\n",
       " 'chick': 817,\n",
       " 'disappointing': 818,\n",
       " 'lousy': 819,\n",
       " 'bring': 820,\n",
       " 'fresh': 821,\n",
       " 'bold': 822,\n",
       " 'helps': 823,\n",
       " 'idiot': 824,\n",
       " 'occupied': 825,\n",
       " 'instead': 826,\n",
       " 'accused': 827,\n",
       " 'edge': 828,\n",
       " 'somewhat': 829,\n",
       " 'afraid': 830,\n",
       " 'advise': 831,\n",
       " 'tremendously': 832,\n",
       " 'sandra': 833,\n",
       " 'bullock': 834,\n",
       " 'supposedly': 835,\n",
       " 'several': 836,\n",
       " 'moments': 837,\n",
       " 'need': 838,\n",
       " 'friends': 839,\n",
       " 'disappointment': 840,\n",
       " 'cannot': 841,\n",
       " 'stand': 842,\n",
       " 'losing': 843,\n",
       " 'putting': 844,\n",
       " 'ratings': 845,\n",
       " 'dramatic': 846,\n",
       " 'tension': 847,\n",
       " 'central': 848,\n",
       " 'themes': 849,\n",
       " 'handled': 850,\n",
       " 'pictures': 851,\n",
       " 'flawed': 852,\n",
       " 'core': 853,\n",
       " 'following': 854,\n",
       " 'bunch': 855,\n",
       " 'high': 856,\n",
       " 'hell': 857,\n",
       " 'viewing': 858,\n",
       " 'disaster': 859,\n",
       " 'paid': 860,\n",
       " 'quinn': 861,\n",
       " 'crazy': 862,\n",
       " 'hate': 863,\n",
       " 'yeah': 864,\n",
       " 'girlfriend': 865,\n",
       " 'disliked': 866,\n",
       " 'five': 867,\n",
       " 'mad': 868,\n",
       " '50': 869,\n",
       " 'cardboard': 870,\n",
       " 'predictably': 871,\n",
       " 'crafted': 872,\n",
       " 'depressing': 873,\n",
       " 'racism': 874,\n",
       " 'took': 875,\n",
       " 'redeeming': 876,\n",
       " 'appalling': 877,\n",
       " 'setting': 878,\n",
       " 'charisma': 879,\n",
       " 'explanation': 880,\n",
       " 'era': 881,\n",
       " 'wanted': 882,\n",
       " 'missed': 883,\n",
       " 'step': 884,\n",
       " 'freedom': 885,\n",
       " 'below': 886,\n",
       " 'received': 887,\n",
       " 'wayne': 888,\n",
       " 'industry': 889,\n",
       " 'noteworthy': 890,\n",
       " 'blood': 891,\n",
       " 'jamie': 892,\n",
       " 'genius': 893,\n",
       " 'owned': 894,\n",
       " 'daughter': 895,\n",
       " 'material': 896,\n",
       " 'goes': 897,\n",
       " 'hence': 898,\n",
       " 'machine': 899,\n",
       " 'flaws': 900,\n",
       " 'mishima': 901,\n",
       " 'uninteresting': 902,\n",
       " 'chilly': 903,\n",
       " 'schrader': 904,\n",
       " 'recently': 905,\n",
       " 'struck': 906,\n",
       " 'contained': 907,\n",
       " 'realistic': 908,\n",
       " 'lacked': 909,\n",
       " 'talent': 910,\n",
       " \"director's\": 911,\n",
       " 'chance': 912,\n",
       " 'master': 913,\n",
       " 'thrilled': 914,\n",
       " 'senses': 915,\n",
       " 'deeply': 916,\n",
       " 'june': 917,\n",
       " 'considering': 918,\n",
       " 'offers': 919,\n",
       " 'mexican': 920,\n",
       " 'matter': 921,\n",
       " 'noir': 922,\n",
       " 'complex': 923,\n",
       " 'psychological': 924,\n",
       " 'soul': 925,\n",
       " 'water': 926,\n",
       " 'gripping': 927,\n",
       " 'control': 928,\n",
       " 'disturbing': 929,\n",
       " 'jerky': 930,\n",
       " 'camerawork': 931,\n",
       " 'sick': 932,\n",
       " 'summary': 933,\n",
       " \"weren't\": 934,\n",
       " 'witty': 935,\n",
       " 'above': 936,\n",
       " 'ceases': 937,\n",
       " 'directors': 938,\n",
       " 'visually': 939,\n",
       " 'spoiler': 940,\n",
       " 'remaining': 941,\n",
       " 'suffering': 942,\n",
       " 'smile': 943,\n",
       " 'literally': 944,\n",
       " '25': 945,\n",
       " 'unfolds': 946,\n",
       " 'leaves': 947,\n",
       " 'room': 948,\n",
       " 'contrast': 949,\n",
       " 'sublime': 950,\n",
       " '5': 951,\n",
       " 'offensive': 952,\n",
       " 'poetry': 953,\n",
       " 'reviewer': 954,\n",
       " 'masterful': 955,\n",
       " 'pitiful': 956,\n",
       " 'nature': 957,\n",
       " 'female': 958,\n",
       " 'nuts': 959,\n",
       " 'dangerous': 960,\n",
       " 'reactions': 961,\n",
       " 'twist': 962,\n",
       " 'shed': 963,\n",
       " 'forget': 964,\n",
       " 'underneath': 965,\n",
       " 'call': 966,\n",
       " 'assistant': 967,\n",
       " 'laugh': 968,\n",
       " 'de': 969,\n",
       " 'cute': 970,\n",
       " 'guy': 971,\n",
       " \"he's\": 972,\n",
       " 'taped': 973,\n",
       " 'likes': 974,\n",
       " 'wholesome': 975,\n",
       " 'ways': 976,\n",
       " 'portraying': 977,\n",
       " 'pleased': 978,\n",
       " 'modern': 979,\n",
       " 'taking': 980,\n",
       " 'towards': 981,\n",
       " 'attempts': 982,\n",
       " 'supposed': 983,\n",
       " 'light': 984,\n",
       " 'situation': 985,\n",
       " 'leave': 986,\n",
       " 'bear': 987,\n",
       " 'kinda': 988,\n",
       " 'question': 989,\n",
       " 'lucy': 990,\n",
       " 'wonder': 991,\n",
       " 'composition': 992,\n",
       " 'brian': 993,\n",
       " 'son': 994,\n",
       " 'member': 995,\n",
       " 'identify': 996,\n",
       " \"huston's\": 997,\n",
       " 'steve': 998,\n",
       " 'robert': 999,\n",
       " 'father': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.index_word[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tk, 'tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = tk.texts_to_sequences(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 27, 27, 27, 287, 407, 1217, 13, 36, 4, 1218, 1219, 408, 142]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'very', 'slow')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.index_word[4], tk.index_word[27], tk.index_word[287]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, len(seq) - 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[4, 27, 27, 27], 287],\n",
       " [[27, 27, 27, 287], 407],\n",
       " [[27, 27, 287, 407], 1217],\n",
       " [[27, 287, 407, 1217], 13],\n",
       " [[287, 407, 1217, 13], 36]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for seq in seqs:\n",
    "    for i in range(0, len(seq) - 4):\n",
    "        data.append([seq[i:i+4], seq[i+4]])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 27, 27, 27, 287, 407, 1217, 13, 36, 4, 1218, 1219, 408, 142]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 30, 418], 173]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(data)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([x for x, y in data])\n",
    "ys = np.array([y for x, y in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    1,   30,  418],\n",
       "       [1034,  734,  407,  282],\n",
       "       [   1,    1,   29,  477],\n",
       "       ...,\n",
       "       [   4,  512,    3,    1],\n",
       "       [  24,  168,   66,   44],\n",
       "       [   2,  120,    6,   30]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173,   3, 260, ..., 379,  35,  25])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lm-data.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((xs, ys), 'lm-data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = joblib.load('tokenizer.pkl')\n",
    "xs, ys = joblib.load('lm-data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORD = tk.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = tf.keras.layers.Embedding(\n",
    "    input_dim = NUM_WORD,\n",
    "    output_dim= 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 8)           16008     \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 8)                0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2001)              18009     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,089\n",
      "Trainable params: 34,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm = tf.keras.models.Sequential([\n",
    "    emb1,\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(NUM_WORD)\n",
    "])\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "83/83 [==============================] - 3s 9ms/step - loss: 8.7204 - accuracy: 0.0398\n",
      "Epoch 2/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 3/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 4/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 5/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 6/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 7/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 8/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 9/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 10/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 11/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 12/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 13/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 14/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 15/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 16/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 17/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 18/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 19/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 20/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 21/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 22/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 23/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 24/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 25/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 26/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 27/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 28/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 29/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 30/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 31/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 32/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 33/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 34/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 35/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 36/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 37/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 38/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 39/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 40/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 41/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 42/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 43/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 44/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 45/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 46/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 47/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 48/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 49/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 50/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 51/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 52/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 53/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 54/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 55/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 56/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 57/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 58/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 59/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 60/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 61/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 62/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 63/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 64/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 65/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 66/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 67/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 68/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 69/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 70/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 71/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 72/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 73/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 74/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 75/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 76/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 77/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 78/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 79/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 80/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 81/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 82/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 83/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 84/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 85/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 86/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 87/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 88/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 89/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 90/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 91/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 92/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 93/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 94/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 95/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 96/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 97/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 98/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 99/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 100/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 101/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 102/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 103/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 104/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 105/200\n",
      "83/83 [==============================] - 1s 18ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 106/200\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 107/200\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 108/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 109/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 110/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 111/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 112/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 113/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 114/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 115/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 116/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 117/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 118/200\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 119/200\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 120/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 121/200\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 122/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 123/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 124/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 125/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 126/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 127/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 128/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 129/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 130/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 131/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 132/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 133/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 134/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 135/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 136/200\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 137/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 138/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 139/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 140/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 141/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 142/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 143/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 144/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 145/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 146/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 147/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 148/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 149/200\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 150/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 151/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 152/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 153/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 154/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 155/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 156/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 157/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 158/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 159/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 160/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 161/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 162/200\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 163/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 164/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 165/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 166/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 167/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 168/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 169/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 170/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 171/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 172/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 173/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 174/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 175/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 176/200\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 177/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 178/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 179/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 180/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 181/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 182/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 183/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 184/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 185/200\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 186/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 187/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 188/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 189/200\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 190/200\n",
      "83/83 [==============================] - 1s 15ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 191/200\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 192/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 193/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 194/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 195/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 196/200\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 197/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 198/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 199/200\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 7.6014 - accuracy: 0.0011\n",
      "Epoch 200/200\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 7.6014 - accuracy: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd7f30a080>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.compile('adam', 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "lm.fit(xs, ys, batch_size=128, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lm.krs\\assets\n"
     ]
    }
   ],
   "source": [
    "lm.save('lm.krs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x1dd7f4bee60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'embedding/embeddings:0' shape=(2001, 8) dtype=float32, numpy=\n",
       "array([[ 0.00636067, -0.02665342,  0.0174864 , ..., -0.01197475,\n",
       "         0.01596201, -0.00140953],\n",
       "       [-0.02561057,  0.0160512 , -0.00714912, ...,  0.01661459,\n",
       "        -0.00476409, -0.02121273],\n",
       "       [-0.00139477,  0.04128186,  0.0097001 , ...,  0.00030349,\n",
       "        -0.00086606, -0.02254425],\n",
       "       ...,\n",
       "       [ 0.04123129, -0.00050108,  0.03758924, ..., -0.00742885,\n",
       "        -0.0081973 ,  0.03211562],\n",
       "       [-0.0392708 , -0.0067884 , -0.0138414 , ..., -0.00747928,\n",
       "         0.03647743,  0.04619696],\n",
       "       [-0.03103733, -0.0334244 , -0.03093401, ...,  0.00289081,\n",
       "         0.01166707,  0.03673227]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00636067, -0.02665342,  0.0174864 , ..., -0.01197475,\n",
       "         0.01596201, -0.00140953],\n",
       "       [-0.02561057,  0.0160512 , -0.00714912, ...,  0.01661459,\n",
       "        -0.00476409, -0.02121273],\n",
       "       [-0.00139477,  0.04128186,  0.0097001 , ...,  0.00030349,\n",
       "        -0.00086606, -0.02254425],\n",
       "       ...,\n",
       "       [ 0.04123129, -0.00050108,  0.03758924, ..., -0.00742885,\n",
       "        -0.0081973 ,  0.03211562],\n",
       "       [-0.0392708 , -0.0067884 , -0.0138414 , ..., -0.00747928,\n",
       "         0.03647743,  0.04619696],\n",
       "       [-0.03103733, -0.0334244 , -0.03093401, ...,  0.00289081,\n",
       "         0.01166707,  0.03673227]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = emb1.embeddings.numpy()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00636067, -0.02665342,  0.0174864 , ..., -0.01197475,\n",
       "         0.01596201, -0.00140953],\n",
       "       [-0.02561057,  0.0160512 , -0.00714912, ...,  0.01661459,\n",
       "        -0.00476409, -0.02121273],\n",
       "       [-0.00139477,  0.04128186,  0.0097001 , ...,  0.00030349,\n",
       "        -0.00086606, -0.02254425],\n",
       "       ...,\n",
       "       [ 0.04123129, -0.00050108,  0.03758924, ..., -0.00742885,\n",
       "        -0.0081973 ,  0.03211562],\n",
       "       [-0.0392708 , -0.0067884 , -0.0138414 , ..., -0.00747928,\n",
       "         0.03647743,  0.04619696],\n",
       "       [-0.03103733, -0.0334244 , -0.03093401, ...,  0.00289081,\n",
       "         0.01166707,  0.03673227]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = emb1.get_weights()[0]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(e, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('word-emb.npz', emb=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<unk>', 'so', \"i'll\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tk.index_word[i] for i in xs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   1,  30, 418]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = xs[0].reshape(1,-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2001)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = lm.predict(x.astype('float32'))\n",
    "logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00641964, -0.00474065, -0.00600547, ..., -0.00690872,\n",
       "         -0.00730503, -0.00834102]], dtype=float32),\n",
       " 2001)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit, len(logit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.750704"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00050023, 0.00050107, 0.00050043, ..., 0.00049998, 0.00049978,\n",
       "        0.00049927]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tf.nn.softmax(logit).numpy()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 0.0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.argmax(), logit[0, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.index_word[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 8)           16008     \n",
      "                                                                 \n",
      " global_average_pooling1d_3   (None, 8)                0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,089\n",
      "Trainable params: 16,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb2 = Embedding(\n",
    "    input_dim = tk.num_words + 1,\n",
    "    output_dim= 8,\n",
    "    embeddings_initializer = tf.keras.initializers.Constant(e) \n",
    ")\n",
    "\n",
    "model2 = Sequential([\n",
    "    emb2,\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pads = tf.keras.preprocessing.sequence.pad_sequences(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 2s 10ms/step - loss: 0.6923 - acc: 0.5175 - val_loss: 0.6968 - val_acc: 0.4300\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6903 - acc: 0.5213 - val_loss: 0.6953 - val_acc: 0.4250\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6876 - acc: 0.5225 - val_loss: 0.6942 - val_acc: 0.4350\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6835 - acc: 0.5462 - val_loss: 0.6908 - val_acc: 0.4750\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6759 - acc: 0.6087 - val_loss: 0.6940 - val_acc: 0.4600\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6645 - acc: 0.5950 - val_loss: 0.6816 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6426 - acc: 0.6700 - val_loss: 0.6627 - val_acc: 0.6450\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6130 - acc: 0.7688 - val_loss: 0.6654 - val_acc: 0.5300\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5761 - acc: 0.7900 - val_loss: 0.6286 - val_acc: 0.6850\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.5302 - acc: 0.8450 - val_loss: 0.6104 - val_acc: 0.6850\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.4816 - acc: 0.8813 - val_loss: 0.5961 - val_acc: 0.6900\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.4333 - acc: 0.9050 - val_loss: 0.5556 - val_acc: 0.7600\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.3934 - acc: 0.9187 - val_loss: 0.5405 - val_acc: 0.7600\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.3513 - acc: 0.9337 - val_loss: 0.5209 - val_acc: 0.7700\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.3152 - acc: 0.9400 - val_loss: 0.4946 - val_acc: 0.7800\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2848 - acc: 0.9450 - val_loss: 0.5241 - val_acc: 0.7450\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2615 - acc: 0.9488 - val_loss: 0.4948 - val_acc: 0.7600\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2326 - acc: 0.9625 - val_loss: 0.4722 - val_acc: 0.7750\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.2144 - acc: 0.9600 - val_loss: 0.4865 - val_acc: 0.7650\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1954 - acc: 0.9650 - val_loss: 0.4504 - val_acc: 0.7900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de0b6bb850>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile('adam', 'binary_crossentropy', ['acc'])\n",
    "model2.fit(pads, df.sentiment, batch_size=8, epochs=20, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "334544a737ead5017040ac753f52220319955d2381f512ab105ce194db781c37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
